2024-12-07 00:08:00,775 - __main__ - INFO - Starting enhanced model training process...
2024-12-07 00:08:06,031 - __main__ - ERROR - Error in load_data: An error occurred while calling o36.collectToPython.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:110)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:293)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:293)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 62 more

2024-12-07 00:08:06,092 - __main__ - ERROR - Error in model training: An error occurred while calling o36.collectToPython.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:110)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:293)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:293)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 62 more

2024-12-07 00:08:06,092 - __main__ - ERROR - Error in main: An error occurred while calling o36.collectToPython.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:110)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:107)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:293)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:293)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 62 more

2024-12-07 00:08:06,096 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:04,204 - __main__ - INFO - Starting enhanced model training process...
2024-12-07 00:16:11,566 - __main__ - INFO - Successfully loaded tables:
2024-12-07 00:16:11,567 - __main__ - INFO - Revenue: (25, 6)
2024-12-07 00:16:11,567 - __main__ - INFO - Discount: (25, 7)
2024-12-07 00:16:11,567 - __main__ - INFO - Price Correlation: (25, 6)
2024-12-07 00:16:11,567 - __main__ - INFO - Reviews: (25, 7)
2024-12-07 00:16:11,568 - __main__ - INFO - Found categories: ['ART AND CRAFTING MATERIALS', 'BATH AND BODY', 'BEDSHEETS', 'BICYCLES', 'BLANKETS', 'BOOKS', 'CANDLES', 'CLOTHING', 'COATS AND JACKETS', 'COOKWARE', 'DRINKWARE', 'ELECTRONICS', 'HOME & KITCHEN', 'MAKEUP', 'MOBILE PHONE ACCESSORIES', 'MOTOR VEHICLE PARTS', 'NAIL CARE', 'PERFUME AND COLOGNE', 'POSTERS AND ARTWORK', 'SKIN CARE', 'SOCKS', 'TOYS & GAMES', 'UNDERWEAR', 'VITAMINS AND SUPPLEMENTS', 'WINE']
2024-12-07 00:16:11,579 - __main__ - INFO - Final merged dataset shape: (25, 23)
2024-12-07 00:16:11,579 - __main__ - INFO - Available columns: ['Category', 'TotalRevenue', 'TotalSales', 'AveragePrice', 'ProductCount', 'AvgDiscountPercentage', 'AvgDiscountPercentage_disc', 'TotalDiscountedRevenue', 'PotentialRevenue', 'AvgSales', 'RevenueLossFromDiscount', 'DiscountROI', 'PriceRatingCorrelation', 'AvgPrice', 'AvgRating', 'AvgReviews', 'PriceRatingRelationship', 'TotalReviews', 'TotalSales_review', 'AvgRating_review', 'HighRatingCount', 'ReviewToSalesRatio', 'CustomerEngagementLevel']
2024-12-07 00:16:11,832 - __main__ - INFO - Created advanced features. Shape: (25, 44)
2024-12-07 00:16:11,833 - __main__ - INFO - Number of numeric features: 23
2024-12-07 00:16:11,833 - __main__ - INFO - Number of categorical features: 3
2024-12-07 00:16:11,837 - __main__ - INFO - Training stacked model...
2024-12-07 00:16:11,872 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,883 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,888 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,894 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,900 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,905 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,913 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,921 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,932 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,942 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,951 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,964 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,979 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:11,998 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:12,067 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:12,108 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:12,119 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:12,744 - py4j.clientserver - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer
2024-12-07 00:16:12,746 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:12,746 - root - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2024-12-07 00:16:12,749 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:14,496 - __main__ - INFO - Model Performance:
2024-12-07 00:16:14,496 - __main__ - INFO - R² score: 0.9832
2024-12-07 00:16:14,496 - __main__ - INFO - MSE: 5020.8544
2024-12-07 00:16:14,496 - __main__ - INFO - RMSE: 70.8580
2024-12-07 00:16:14,497 - __main__ - INFO - MAE: 60.7211
2024-12-07 00:16:14,497 - __main__ - WARNING - Feature importance length mismatch: 3 vs 53
2024-12-07 00:16:14,498 - __main__ - INFO - 
Top 10 Most Important Features:
2024-12-07 00:16:14,498 - __main__ - INFO -                  feature  importance  importance_percentage
1  AvgDiscountPercentage    0.541399              54.139880
0           AveragePrice    0.408668              40.866799
2              AvgRating    0.049933               4.993321
2024-12-07 00:16:14,548 - __main__ - INFO - Model saved to /home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/src/models/training/trained_models/sales_predictor.joblib
2024-12-07 00:16:14,833 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:16:59,031 - __main__ - INFO - Starting enhanced model training process...
2024-12-07 00:17:07,819 - __main__ - INFO - Successfully loaded tables:
2024-12-07 00:17:07,820 - __main__ - INFO - Revenue: (25, 6)
2024-12-07 00:17:07,820 - __main__ - INFO - Discount: (25, 7)
2024-12-07 00:17:07,820 - __main__ - INFO - Price Correlation: (25, 6)
2024-12-07 00:17:07,820 - __main__ - INFO - Reviews: (25, 7)
2024-12-07 00:17:07,820 - __main__ - INFO - Found categories: ['ART AND CRAFTING MATERIALS', 'BATH AND BODY', 'BEDSHEETS', 'BICYCLES', 'BLANKETS', 'BOOKS', 'CANDLES', 'CLOTHING', 'COATS AND JACKETS', 'COOKWARE', 'DRINKWARE', 'ELECTRONICS', 'HOME & KITCHEN', 'MAKEUP', 'MOBILE PHONE ACCESSORIES', 'MOTOR VEHICLE PARTS', 'NAIL CARE', 'PERFUME AND COLOGNE', 'POSTERS AND ARTWORK', 'SKIN CARE', 'SOCKS', 'TOYS & GAMES', 'UNDERWEAR', 'VITAMINS AND SUPPLEMENTS', 'WINE']
2024-12-07 00:17:07,824 - __main__ - INFO - Final merged dataset shape: (25, 23)
2024-12-07 00:17:07,824 - __main__ - INFO - Available columns: ['Category', 'TotalRevenue', 'TotalSales', 'AveragePrice', 'ProductCount', 'AvgDiscountPercentage', 'AvgDiscountPercentage_disc', 'TotalDiscountedRevenue', 'PotentialRevenue', 'AvgSales', 'RevenueLossFromDiscount', 'DiscountROI', 'PriceRatingCorrelation', 'AvgPrice', 'AvgRating', 'AvgReviews', 'PriceRatingRelationship', 'TotalReviews', 'TotalSales_review', 'AvgRating_review', 'HighRatingCount', 'ReviewToSalesRatio', 'CustomerEngagementLevel']
2024-12-07 00:17:08,131 - __main__ - INFO - Created advanced features. Shape: (25, 44)
2024-12-07 00:17:08,131 - __main__ - INFO - Number of numeric features: 23
2024-12-07 00:17:08,131 - __main__ - INFO - Number of categorical features: 3
2024-12-07 00:17:08,133 - __main__ - INFO - Training stacked model...
2024-12-07 00:17:08,161 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,172 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,177 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,182 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,189 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,195 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,201 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,208 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,214 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,220 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,229 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,237 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,246 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,256 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,283 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,282 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,311 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,777 - py4j.clientserver - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer
2024-12-07 00:17:08,779 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:08,779 - root - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2024-12-07 00:17:08,780 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:17:10,507 - __main__ - INFO - Model Performance:
2024-12-07 00:17:10,508 - __main__ - INFO - R² score: 0.9832
2024-12-07 00:17:10,508 - __main__ - INFO - MSE: 5020.8544
2024-12-07 00:17:10,508 - __main__ - INFO - RMSE: 70.8580
2024-12-07 00:17:10,508 - __main__ - INFO - MAE: 60.7211
2024-12-07 00:17:10,509 - __main__ - WARNING - Feature importance length mismatch: 3 vs 53
2024-12-07 00:17:10,510 - __main__ - INFO - 
Top 10 Most Important Features:
2024-12-07 00:17:10,510 - __main__ - INFO -                  feature  importance  importance_percentage
1  AvgDiscountPercentage    0.541399              54.139880
0           AveragePrice    0.408668              40.866799
2              AvgRating    0.049933               4.993321
2024-12-07 00:17:10,556 - __main__ - INFO - Model saved to /home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/src/models/training/trained_models/sales_predictor.joblib
2024-12-07 00:17:10,848 - py4j.clientserver - INFO - Closing down clientserver connection
