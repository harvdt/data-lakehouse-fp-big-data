2024-12-06 20:22:53,486 - main - ERROR - Error loading ML model: [Errno 2] No such file or directory: 'trained_models/sales_predictor.joblib'
2024-12-06 20:22:53,486 - main - ERROR - Error loading ML model: [Errno 2] No such file or directory: 'trained_models/sales_predictor.joblib'
2024-12-06 20:22:53,486 - main - ERROR - Error loading ML model: [Errno 2] No such file or directory: 'trained_models/sales_predictor.joblib'
2024-12-06 20:22:53,486 - main - ERROR - Error loading ML model: [Errno 2] No such file or directory: 'trained_models/sales_predictor.joblib'
2024-12-06 20:24:10,511 - main - ERROR - Error reading Delta table revenue_per_category: An error occurred while calling o31.load.
: java.lang.ClassCastException: class org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus cannot be cast to class org.apache.spark.sql.execution.datasources.FileStatusWithMetadata (org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus and org.apache.spark.sql.execution.datasources.FileStatusWithMetadata are in unnamed module of loader 'app')
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2(DataSourceScanExec.scala:466)
	at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2$adapted(DataSourceScanExec.scala:466)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.execution.FileSourceScanLike.setFilesNumAndSizeMetric(DataSourceScanExec.scala:466)
	at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions(DataSourceScanExec.scala:257)
	at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions$(DataSourceScanExec.scala:251)
	at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions$lzycompute(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions(DataSourceScanExec.scala:286)
	at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions$(DataSourceScanExec.scala:267)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions$lzycompute(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:553)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:141)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:141)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:146)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)
	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
	at org.apache.spark.sql.delta.util.StateCache$CachedDS.<init>(StateCache.scala:57)
	at org.apache.spark.sql.delta.util.StateCache.$anonfun$cacheDS$1(StateCache.scala:107)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.util.StateCache.cacheDS(StateCache.scala:107)
	at org.apache.spark.sql.delta.util.StateCache.cacheDS$(StateCache.scala:106)
	at org.apache.spark.sql.delta.Snapshot.cacheDS(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$cachedState$1(Snapshot.scala:165)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.cachedState$lzycompute(Snapshot.scala:165)
	at org.apache.spark.sql.delta.Snapshot.cachedState(Snapshot.scala:164)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$stateDF$1(Snapshot.scala:175)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.stateDF(Snapshot.scala:175)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$3(Snapshot.scala:220)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$2(Snapshot.scala:216)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$1(Snapshot.scala:216)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)
	at org.apache.spark.sql.delta.Snapshot.withStatusCode(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.computedState$lzycompute(Snapshot.scala:215)
	at org.apache.spark.sql.delta.Snapshot.computedState(Snapshot.scala:213)
	at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:253)
	at org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$init$(DataSkippingReader.scala:178)
	at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:68)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$1(SnapshotManagement.scala:300)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:440)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:428)
	at org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:290)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:285)
	at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:271)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:264)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:262)
	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:260)
	at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)
	at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:68)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:593)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:589)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)
	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)
	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)
	at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)
	at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:588)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:604)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
	at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:604)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:611)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:507)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:83)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:82)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:113)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:113)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:101)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:164)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:209)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:167)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-12-06 20:24:10,512 - main - ERROR - Error fetching categories: 
2024-12-06 20:25:13,305 - main - ERROR - Prediction error: 
2024-12-06 22:41:06,604 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-06 22:42:38,839 - main - INFO - ML model loaded successfully
2024-12-06 22:42:38,840 - main - INFO - ML model loaded successfully
2024-12-06 22:42:38,842 - main - INFO - ML model loaded successfully
2024-12-06 22:42:38,842 - main - INFO - ML model loaded successfully
2024-12-06 22:43:43,112 - main - ERROR - Error reading Delta table revenue_per_category: An error occurred while calling o35.load.
: java.lang.ClassCastException: class org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus cannot be cast to class org.apache.spark.sql.execution.datasources.FileStatusWithMetadata (org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus and org.apache.spark.sql.execution.datasources.FileStatusWithMetadata are in unnamed module of loader 'app')
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2(DataSourceScanExec.scala:466)
	at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2$adapted(DataSourceScanExec.scala:466)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.execution.FileSourceScanLike.setFilesNumAndSizeMetric(DataSourceScanExec.scala:466)
	at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions(DataSourceScanExec.scala:257)
	at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions$(DataSourceScanExec.scala:251)
	at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions$lzycompute(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions(DataSourceScanExec.scala:286)
	at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions$(DataSourceScanExec.scala:267)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions$lzycompute(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:553)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:141)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:141)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:146)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)
	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
	at org.apache.spark.sql.delta.util.StateCache$CachedDS.<init>(StateCache.scala:57)
	at org.apache.spark.sql.delta.util.StateCache.$anonfun$cacheDS$1(StateCache.scala:107)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.util.StateCache.cacheDS(StateCache.scala:107)
	at org.apache.spark.sql.delta.util.StateCache.cacheDS$(StateCache.scala:106)
	at org.apache.spark.sql.delta.Snapshot.cacheDS(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$cachedState$1(Snapshot.scala:165)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.cachedState$lzycompute(Snapshot.scala:165)
	at org.apache.spark.sql.delta.Snapshot.cachedState(Snapshot.scala:164)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$stateDF$1(Snapshot.scala:175)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.stateDF(Snapshot.scala:175)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$3(Snapshot.scala:220)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$2(Snapshot.scala:216)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$1(Snapshot.scala:216)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)
	at org.apache.spark.sql.delta.Snapshot.withStatusCode(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.computedState$lzycompute(Snapshot.scala:215)
	at org.apache.spark.sql.delta.Snapshot.computedState(Snapshot.scala:213)
	at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:253)
	at org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$init$(DataSkippingReader.scala:178)
	at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:68)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$1(SnapshotManagement.scala:300)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:440)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:428)
	at org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:290)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:285)
	at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:271)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:264)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:262)
	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:260)
	at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)
	at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:68)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:593)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:589)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)
	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)
	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)
	at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)
	at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:588)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:604)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
	at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:604)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:611)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:507)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:83)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:82)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:113)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:113)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:101)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:164)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:209)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:167)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-12-06 22:43:43,396 - main - ERROR - Error fetching categories: 
2024-12-06 22:44:16,642 - main - ERROR - Error reading Delta table revenue_per_category: An error occurred while calling o72.load.
: java.lang.ClassCastException: class org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus cannot be cast to class org.apache.spark.sql.execution.datasources.FileStatusWithMetadata (org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus and org.apache.spark.sql.execution.datasources.FileStatusWithMetadata are in unnamed module of loader 'app')
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2(DataSourceScanExec.scala:466)
	at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2$adapted(DataSourceScanExec.scala:466)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.execution.FileSourceScanLike.setFilesNumAndSizeMetric(DataSourceScanExec.scala:466)
	at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions(DataSourceScanExec.scala:257)
	at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions$(DataSourceScanExec.scala:251)
	at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions$lzycompute(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions(DataSourceScanExec.scala:286)
	at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions$(DataSourceScanExec.scala:267)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions$lzycompute(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions(DataSourceScanExec.scala:506)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:553)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:141)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:141)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:146)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)
	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
	at org.apache.spark.sql.delta.util.StateCache$CachedDS.<init>(StateCache.scala:57)
	at org.apache.spark.sql.delta.util.StateCache.$anonfun$cacheDS$1(StateCache.scala:107)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.util.StateCache.cacheDS(StateCache.scala:107)
	at org.apache.spark.sql.delta.util.StateCache.cacheDS$(StateCache.scala:106)
	at org.apache.spark.sql.delta.Snapshot.cacheDS(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$cachedState$1(Snapshot.scala:165)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.cachedState$lzycompute(Snapshot.scala:165)
	at org.apache.spark.sql.delta.Snapshot.cachedState(Snapshot.scala:164)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$stateDF$1(Snapshot.scala:175)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.stateDF(Snapshot.scala:175)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$3(Snapshot.scala:220)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$2(Snapshot.scala:216)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
	at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$1(Snapshot.scala:216)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)
	at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)
	at org.apache.spark.sql.delta.Snapshot.withStatusCode(Snapshot.scala:58)
	at org.apache.spark.sql.delta.Snapshot.computedState$lzycompute(Snapshot.scala:215)
	at org.apache.spark.sql.delta.Snapshot.computedState(Snapshot.scala:213)
	at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:253)
	at org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$init$(DataSkippingReader.scala:178)
	at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:68)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$1(SnapshotManagement.scala:300)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:440)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:428)
	at org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:290)
	at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:285)
	at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:271)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:264)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:262)
	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:260)
	at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:63)
	at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)
	at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:68)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:593)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:589)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)
	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)
	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)
	at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)
	at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:456)
	at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:588)
	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:604)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
	at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:604)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:611)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:507)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:83)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:82)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:113)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:113)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:101)
	at org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:164)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:209)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:167)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-12-06 22:46:10,597 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-06 22:46:34,492 - main - INFO - ML model loaded successfully
2024-12-06 22:46:34,493 - main - INFO - ML model loaded successfully
2024-12-06 22:46:34,495 - main - INFO - ML model loaded successfully
2024-12-06 22:46:34,496 - main - INFO - ML model loaded successfully
2024-12-06 22:46:56,295 - main - ERROR - Data not found: Data not found at /home/zaki/kuliah/Bigdata/data-lakehouse-fp-big-data/src/data/gold/revenue_per_category_parquet
2024-12-06 22:46:56,295 - main - ERROR - Error fetching categories: 
2024-12-06 23:04:19,335 - main - INFO - ML model loaded successfully
2024-12-06 23:04:19,347 - main - INFO - ML model loaded successfully
2024-12-06 23:04:19,363 - main - INFO - ML model loaded successfully
2024-12-06 23:04:19,371 - main - INFO - ML model loaded successfully
2024-12-06 23:07:14,156 - main - INFO - ML model loaded successfully
2024-12-06 23:07:14,183 - main - INFO - ML model loaded successfully
2024-12-06 23:07:14,199 - main - INFO - ML model loaded successfully
2024-12-06 23:07:14,212 - main - INFO - ML model loaded successfully
2024-12-06 23:19:22,258 - main - INFO - ML model loaded successfully
2024-12-06 23:19:22,265 - main - INFO - ML model loaded successfully
2024-12-06 23:19:22,269 - main - INFO - ML model loaded successfully
2024-12-06 23:19:22,298 - main - INFO - ML model loaded successfully
2024-12-06 23:20:53,451 - main - INFO - ML model loaded successfully
2024-12-06 23:20:53,473 - main - INFO - ML model loaded successfully
2024-12-06 23:20:53,487 - main - INFO - ML model loaded successfully
2024-12-06 23:20:53,524 - main - INFO - ML model loaded successfully
2024-12-06 23:37:32,818 - main - INFO - ML model loaded successfully
2024-12-06 23:37:32,838 - main - INFO - ML model loaded successfully
2024-12-06 23:37:32,880 - main - INFO - ML model loaded successfully
2024-12-06 23:37:32,898 - main - INFO - ML model loaded successfully
2024-12-07 00:16:16,976 - main - INFO - ML model loaded successfully
2024-12-07 00:16:17,020 - main - INFO - ML model loaded successfully
2024-12-07 00:16:17,055 - main - INFO - ML model loaded successfully
2024-12-07 00:16:17,068 - main - INFO - ML model loaded successfully
2024-12-07 00:17:12,895 - main - INFO - ML model loaded successfully
2024-12-07 00:17:12,946 - main - INFO - ML model loaded successfully
2024-12-07 00:17:12,978 - main - INFO - ML model loaded successfully
2024-12-07 00:17:13,022 - main - INFO - ML model loaded successfully
2024-12-07 00:20:01,729 - main - ERROR - Prediction error: "['PriceToAvgRatio', 'PriceVariance', 'DiscountEfficiency', 'DiscountEffectiveness', 'ReviewsPerSale', 'HighRatingRatio', 'EngagementScore', 'CustomerSatisfaction', 'RevenuePerProduct', 'SalesEfficiency', 'PriceRatingInteraction', 'DiscountRatingInteraction', 'PriceDiscountInteraction', 'MarketPerformance', 'CustomerEngagementIndex', 'ProfitabilityScore', 'Normalized_TotalRevenue', 'Normalized_TotalSales', 'Normalized_AveragePrice'] not in index"
2024-12-07 00:21:27,493 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:21:50,990 - main - INFO - ML model loaded successfully
2024-12-07 00:21:51,016 - main - INFO - ML model loaded successfully
2024-12-07 00:21:51,032 - main - INFO - ML model loaded successfully
2024-12-07 00:21:51,049 - main - INFO - ML model loaded successfully
2024-12-07 00:23:04,355 - main - ERROR - Prediction error: Bin edges must be unique: array([299.99, 299.99, 299.99, 299.99, 299.99, 299.99]).
You can drop duplicate edges by setting the 'duplicates' kwarg
2024-12-07 00:25:31,187 - main - ERROR - Prediction error: Bin edges must be unique: array([99.99, 99.99, 99.99, 99.99, 99.99, 99.99]).
You can drop duplicate edges by setting the 'duplicates' kwarg
2024-12-07 00:26:25,516 - main - ERROR - Prediction error: Bin edges must be unique: array([299.99, 299.99, 299.99, 299.99, 299.99, 299.99]).
You can drop duplicate edges by setting the 'duplicates' kwarg
2024-12-07 00:28:54,087 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:28:54,087 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-07 00:29:17,347 - main - INFO - ML model loaded successfully
2024-12-07 00:29:17,355 - main - INFO - ML model loaded successfully
2024-12-07 00:29:17,371 - main - INFO - ML model loaded successfully
2024-12-07 00:29:17,398 - main - INFO - ML model loaded successfully
2024-12-08 10:54:26,689 - main - INFO - ML model loaded successfully
2024-12-08 10:54:26,697 - main - INFO - ML model loaded successfully
2024-12-08 10:54:26,707 - main - INFO - ML model loaded successfully
2024-12-08 10:54:26,708 - main - INFO - ML model loaded successfully
2024-12-08 20:04:00,187 - py4j.clientserver - INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2024-12-08 20:04:00,192 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,197 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-12-08 20:04:00,199 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,199 - main - ERROR - Error creating Spark session: SparkConf does not exist in the JVM
2024-12-08 20:04:00,199 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: SparkConf does not exist in the JVM
2024-12-08 20:04:00,801 - py4j.clientserver - INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2024-12-08 20:04:00,801 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,802 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aveee/data-lakehouse-fp-big-data/venv/lib/python3.11/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-12-08 20:04:00,802 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,802 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,802 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,802 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,802 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,802 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,802 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,804 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,805 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,954 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:04:00,954 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:04:00,954 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:06:10,180 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:06:10,193 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:06:10,194 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:06:10,194 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:06:10,194 - main - ERROR - Error fetching categories: 
2024-12-08 20:06:10,199 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:06:10,226 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:06:10,226 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:06:10,226 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:06:10,226 - main - ERROR - Error fetching categories: 
2024-12-08 20:07:17,673 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:17,873 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:17,873 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:17,873 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:17,873 - main - ERROR - Error fetching categories: 
2024-12-08 20:07:17,876 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:17,901 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:17,902 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:17,902 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:17,902 - main - ERROR - Error fetching categories: 
2024-12-08 20:07:17,990 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,013 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,013 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:18,013 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:18,014 - main - ERROR - Error fetching categories: 
2024-12-08 20:07:18,016 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,041 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,041 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:18,041 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:18,043 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,071 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,071 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:18,071 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:18,071 - main - ERROR - Error fetching categories: 
2024-12-08 20:07:18,072 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,099 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,099 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:18,099 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,145 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,146 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,327 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,347 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:18,347 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:18,347 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:19,795 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:19,822 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:19,822 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:19,822 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:07:19,825 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:19,857 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:07:19,857 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:07:19,857 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:08:32,876 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:32,921 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:32,921 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:08:32,921 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:08:32,922 - main - ERROR - Error fetching categories: 
2024-12-08 20:08:37,916 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:38,097 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:38,097 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:08:38,097 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:08:38,100 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:38,132 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:38,132 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:08:38,132 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:08:52,612 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:52,654 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:52,654 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:08:52,654 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:08:57,213 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:57,240 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:08:57,240 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:08:57,240 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:09:00,583 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:00,619 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:00,620 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:09:00,620 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:09:08,076 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:08,102 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:08,102 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:09:08,102 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:09:39,603 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:39,632 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:39,632 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:09:39,632 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:09:56,753 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:56,778 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:09:56,778 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:09:56,778 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:10:21,463 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:10:21,487 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:10:21,487 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:10:21,488 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:12:20,908 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:20,934 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:20,934 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:12:20,934 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:12:39,331 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:39,383 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:39,383 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:12:39,383 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:12:55,021 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:55,046 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:55,046 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:12:55,046 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:12:55,046 - main - ERROR - Error fetching categories: 
2024-12-08 20:12:59,195 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:59,220 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:59,220 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:12:59,220 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:12:59,224 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:59,259 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:12:59,259 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:12:59,259 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:13:14,926 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:13:14,953 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:13:14,953 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:13:14,953 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:27,284 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:27,313 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:27,313 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:27,313 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:27,313 - main - ERROR - Error fetching categories: 
2024-12-08 20:14:30,698 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:30,724 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:30,724 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:30,724 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:34,583 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:34,614 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:34,614 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:34,614 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:34,616 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:34,646 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:34,646 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:34,646 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:54,000 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:54,117 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:54,118 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:54,118 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:54,118 - main - ERROR - Error fetching categories: 
2024-12-08 20:14:54,121 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:54,162 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:54,162 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:54,162 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:54,212 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:54,246 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:54,247 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:54,247 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:58,468 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:58,496 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:58,496 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:58,496 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:58,496 - main - ERROR - Error fetching categories: 
2024-12-08 20:14:58,498 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:58,524 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:14:58,525 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:14:58,525 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:14:58,525 - main - ERROR - Error fetching categories: 
2024-12-08 20:15:02,538 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:02,569 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:02,569 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:02,569 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:15:02,572 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:02,599 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:02,599 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:02,599 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:15:13,827 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:13,861 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:13,861 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:13,861 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:15:18,216 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:18,246 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:18,246 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:18,246 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:15:32,797 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:32,827 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:32,827 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:32,827 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:15:49,523 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:49,660 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:49,660 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:49,660 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:15:49,660 - main - ERROR - Error fetching categories: 
2024-12-08 20:15:50,901 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:50,929 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:50,930 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:50,930 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:15:50,932 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:50,962 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:15:50,962 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:15:50,963 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:16:15,122 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:16:15,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:16:15,147 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:16:15,147 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:17:55,567 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:17:55,593 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:17:55,593 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:17:55,593 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:17:55,593 - main - ERROR - Error fetching categories: 
2024-12-08 20:17:55,595 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:17:55,629 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:17:55,629 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:17:55,629 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:15,525 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:15,558 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:15,558 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:15,558 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:15,558 - main - ERROR - Error fetching categories: 
2024-12-08 20:19:20,221 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:20,242 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:20,242 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:20,242 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:20,243 - main - ERROR - Error fetching categories: 
2024-12-08 20:19:23,073 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:23,117 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:23,117 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:23,118 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:23,120 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:23,265 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:23,265 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:23,265 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:29,052 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,082 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,082 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:29,082 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:29,133 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,160 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,160 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:29,160 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:29,165 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,194 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,194 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:29,194 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:29,818 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,844 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:29,844 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:29,844 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:29,844 - main - ERROR - Error fetching categories: 
2024-12-08 20:19:47,558 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:47,584 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:47,584 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:47,584 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:47,584 - main - ERROR - Error fetching categories: 
2024-12-08 20:19:50,703 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:50,741 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:50,741 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:50,741 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:19:50,744 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:50,771 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:19:50,771 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:19:50,771 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:20:41,764 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:20:41,793 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:20:41,793 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:20:41,793 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:21:35,028 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:21:35,050 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:21:35,051 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:21:35,051 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:22:08,322 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:22:08,474 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:22:08,474 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:22:08,474 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:22:08,477 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:22:08,505 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:22:08,505 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:22:08,505 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:22:19,252 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:22:19,275 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:22:19,275 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:22:19,275 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:22:19,275 - main - ERROR - Error fetching categories: 
2024-12-08 20:23:54,325 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:23:54,361 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:23:54,361 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:23:54,361 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:23:54,365 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:23:54,402 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:23:54,402 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:23:54,402 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:24:13,292 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:13,326 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:13,326 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:24:13,326 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:24:13,332 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:13,365 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:13,365 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:24:13,365 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:24:43,776 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:43,801 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:43,801 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:24:43,801 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:24:43,801 - main - ERROR - Error fetching categories: 
2024-12-08 20:24:43,803 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:43,832 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:43,832 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:24:43,832 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:24:54,340 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:54,360 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:24:54,360 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:24:54,360 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:24:54,360 - main - ERROR - Error fetching categories: 
2024-12-08 20:25:00,695 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:00,844 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:00,844 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:00,844 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:02,166 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:02,204 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:02,204 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:02,204 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:02,206 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:02,240 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:02,240 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:02,240 - main - ERROR - Error reading data for price_rating_correlation: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:18,733 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:18,755 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:18,756 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:18,756 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:34,121 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:34,147 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:34,148 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:34,148 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:34,148 - main - ERROR - Error fetching categories: 
2024-12-08 20:25:34,151 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:34,184 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:34,184 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:34,184 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:34,184 - main - ERROR - Error fetching categories: 
2024-12-08 20:25:46,214 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:46,237 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:46,237 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:46,237 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:46,240 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:46,267 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:46,267 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:46,267 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:58,668 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:58,689 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:58,689 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:58,689 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:25:58,692 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:58,826 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:25:58,826 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:25:58,826 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:26:08,160 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:26:08,185 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:26:08,185 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:26:08,185 - main - ERROR - Error reading data for revenue_per_category: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:26:19,864 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:26:19,889 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:26:19,889 - main - ERROR - Error creating Spark session: [Errno 111] Connection refused
2024-12-08 20:26:19,889 - main - ERROR - Error reading data for discount_effectiveness: Failed to initialize Spark: [Errno 111] Connection refused
2024-12-08 20:26:36,365 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:26:36,365 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:26:36,366 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:26:36,455 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 20:32:57,766 - main - INFO - ML model loaded successfully
2024-12-08 20:32:57,788 - main - INFO - ML model loaded successfully
2024-12-08 20:32:57,794 - main - INFO - ML model loaded successfully
2024-12-08 20:32:57,794 - main - INFO - ML model loaded successfully
2024-12-08 21:12:02,974 - main - INFO - ML model loaded successfully
2024-12-08 21:12:02,980 - main - INFO - ML model loaded successfully
2024-12-08 21:12:02,999 - main - INFO - ML model loaded successfully
2024-12-08 21:12:03,007 - main - INFO - ML model loaded successfully
2024-12-08 23:53:49,300 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 23:53:49,300 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 23:53:49,300 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-08 23:53:49,300 - py4j.clientserver - INFO - Closing down clientserver connection
2024-12-09 09:11:05,195 - main - INFO - ML model loaded successfully
2024-12-09 09:11:05,196 - main - INFO - ML model loaded successfully
2024-12-09 09:11:05,214 - main - INFO - ML model loaded successfully
2024-12-09 09:11:05,225 - main - INFO - ML model loaded successfully
